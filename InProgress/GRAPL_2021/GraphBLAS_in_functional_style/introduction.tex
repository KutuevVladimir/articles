\section{Introduction}

One of the promising ways to high-performance graph analysis is based on the utilization of linear algebra: operations over vectors and matrices can be efficiently implemented on modern parallel hardware, and once we reduce the given graph analysis problem to the composition of such operations, we get a high-performance solution for our problem.
A well-known example of such reduction is a reduction of all-pairs shortest path (APSP) problem to matrix multiplication over appropriate \textit{semiring}.
GraphBLAS API standard~\cite{7761646} provides formalization and generalization of this observation and makes it useful in practice.
GraphBLAS API introduces appropriate algebraic structures (monoid, semiring), objects (scalar, vector, matrix), and operations over them to provide building blocks to create graph analysis algorithms.
It was shown, that sparse linear algebra over specific semirings is useful not only for graph analysis, but also in other areas, such as computational biology~\cite{10.5555/3433701.3433800} and machine learning~\cite{8091098}.

There are a number of GraphBLAS API implementations, such as SuiteSarse:GraphBLAS~\cite{10.1145/3322125} and CombBLAS~\cite{10.1177/1094342011403516}, but all of them do not utilize the power of GPGPU, except GraphBLAST~\cite{10.1145/3466795}, while GPGPU utilization for linear algebra is a common practice today.
GPGPU development is difficult in itself because it introduces heterogeneous computational devices, special programming models, and specific optimizations.
Implementation of GraphBLAS API even more challenging, because it means the processing of irregular data, and the creation of generic (polymorphic) functions to declare and use user-defined semirings which is hard to express in low-level programming languages like CUDA C or OpenCL C which are usually used for GPGPU programming.
Moreover, it is necessary to use high-level optimizations, like kernel fusion or elimination of unnecessary computations to improve the performance of end-user solutions based on the provided API implementation.
But such high-level optimizations are too hard to automate for C-like languages.

Functional programming can help to solve these problems.
First of all, native support functions as parameters simplify semirings descriptions and implementation of functions parametrized with semirings.
Moreover, a powerful type system allows one to describe abstract (generic) functions which simplifies the development and usage of abstract linear algebra operations.
Even more, such native features of functional programming languages, like discriminated unions (union types) and strong static typing allows one to create more robust code.
For example, discriminated unions allows one naturally express \texttt{Min-Plus} semiring, where we should extend $\mathbb{R}$ with special element $\infty$ (infinity, namely identity element for $\oplus$), so we cannot use predefined types like \texttt{float} or \texttt{double}.
Another area where functional programming can be useful is automatic code optimization.
A big number of nontrivial optimizations for functional languages for GPGPU were developed, such as specialization, deforestation, and kernels fusion, one of the actively discussed optimizations in the GraphBLAS community~\cite{10.1145/3466795}.
These techniques make programs in high-level programming languages competitive in terms of performance with solutions written in CUDA or OpenCL C.
For more details one can look at such languages and frameworks as Futhark\footnote{Futhark is a purely functional statically typed programming language for GPGPU. Project web page: \url{https://futhark-lang.org/}. Access date: 08.01.2023.}~\cite{Henriksen:2017:FPF:3062341.3062354}, Accelerate\footnote{Accelerate: GPGPU programming with Haskell. Project web page:\url{https://www.acceleratehs.org/}. Access date: 08.01.2023.}~\cite{10.1145/2544174.2500595}, AnyDSL\footnote{AnyDSL is a partial evaluation framework for parallel programming. Project web page: \url{https://anydsl.github.io/}. Access date: 08.01.2023.}~\cite{10.1145/3276489}.

In this work we discuss a way to implement GraphBLAS API which combines high-performance computations on GPGPU and the power of high-level programming languages in both application development and possible code optimizations.
Our solution is based on metaprogramming techniques: we propose to generate code for GPGPU from a high-level programming language.
Namely, we plan to generate OpenCL C from a subset of  F\# programming language.
To translate F\# to OpenCL C we use a Brahma.FSharp\footnote{Brahma.FSharp project on GitHub: \url{https://github.com/YaccConstructor/Brahma.FSharp}. Access date: 08.01.2023.} which is based on F\# quotations metaprogramming techniques\footnote{F\# code quotations is a runtime metaprogramming technique which allows one to transform written F\# code during program execution. Official documentation: \url{https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/code-quotations}. Access date: 08.01.2023.}.
Usage of F\# simplifies both implementation of GraphBLAS API, making features of functional programming available, and its utilization in application development with high-level programming language on .NET platform.
Moreover, as far as F\# is a functional-first programming language, it should make it possible to use advanced optimization techniques and power of type system.
OpenCL C as a target language provides high portability: it is possible to run OpenCL C code on multi-thread CPU, on different GPGPUs (not only Nvidia but also Intel and AMD).
%, and even on FPGA~\cite{kenter2019invited, 6567546}.

To summarize, we make the following contribution.
\begin{enumerate}
   \item We propose a typing scheme for sparse matrices and show how types can naturally solve the \textit{problem of explicit and implicit zeroes} which actively discussed in community\footnote{Discussions related to \textit{explicit zeroes problem}: \url{https://github.com/lessthanoptimal/ejml/pull/145\#issuecomment-888293732}, \url{https://github.com/GraphBLAS/LAGraph/issues/28}. Access date: 08.01.2023.}.
   \item We show that utilization of generic high-order functions can simplify API by kernels unification.
   \item We implemented generic kernels for matrix-matrix element-wise functions using Brahma.FSharp.
   Evaluation of implemented kernels shows that it is possible to create performant flexible linear algebraic operations using functional programming features and metaprogramming techniques.
\end{enumerate}



