@inproceedings{Henriksen:2017:FPF:3062341.3062354,
 author = {Henriksen, Troels and Serup, Niels G. W. and Elsman, Martin and Henglein, Fritz and Oancea, Cosmin E.},
 title = {Futhark: Purely Functional GPU-programming with Nested Parallelism and In-place Array Updates},
 booktitle = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation},
 series = {PLDI 2017},
 year = {2017},
 isbn = {978-1-4503-4988-8},
 location = {Barcelona, Spain},
 pages = {556--571},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3062341.3062354},
 doi = {10.1145/3062341.3062354},
 acmid = {3062354},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPGPU, compilers, functional language, parallel},
}

@INPROCEEDINGS{7761646,
author={J. {Kepner} and P. {Aaltonen} and D. {Bader} and A. {Buluc} and F. {Franchetti} and J. {Gilbert} and D. {Hutchison} and M. {Kumar} and A. {Lumsdaine} and H. {Meyerhenke} and S. {McMillan} and C. {Yang} and J. D. {Owens} and M. {Zalewski} and T. {Mattson} and J. {Moreira}},
booktitle={2016 IEEE High Performance Extreme Computing Conference (HPEC)},
title={Mathematical foundations of the GraphBLAS},
year={2016},
volume={},
number={},
pages={1--9},
keywords={graph theory;mathematics computing;matrix algebra;programming environments;mathematical foundations;GraphBLAS standard;GraphBlas.org;matrix-based graph algorithms;matrix-based graph operations;programming environments;adjacency matrices;incidence matrices;matrix multiplication;matrix mathematics;Matrices;Sparse matrices;Finite element analysis;Standards;Additives},
doi={10.1109/HPEC.2016.7761646},
ISSN={},
month={Sep.},}

@article{10.1145/3466795,
author = {Yang, Carl and Bulu\c{c}, Ayd\i{}n and Owens, John D.},
title = {GraphBLAST: A High-Performance Linear Algebra-Based Graph Framework on the GPU},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/3466795},
doi = {10.1145/3466795},
abstract = {High-performance implementations of graph algorithms are challenging to implement on new parallel hardware such as GPUs because of three challenges: (1)&nbsp;the difficulty of coming up with graph building blocks, (2)&nbsp;load imbalance on parallel hardware, and (3)&nbsp;graph problems having low arithmetic intensity. To address some of these challenges, GraphBLAS is an innovative, on-going effort by the graph analytics community to propose building blocks based on sparse linear algebra, which allow graph algorithms to be expressed in a performant, succinct, composable, and portable manner. In this paper, we examine the performance challenges of a linear-algebra-based approach to building graph frameworks and describe new design principles for overcoming these bottlenecks. Among the new design principles is exploiting input sparsity, which allows users to write graph algorithms without specifying push and pull direction. Exploiting output sparsity allows users to tell the backend which values of the output in a single vectorized computation they do not want computed. Load-balancing is an important feature for balancing work amongst parallel workers. We describe the important load-balancing features for handling graphs with different characteristics. The design principles described in this paper have been implemented in “GraphBLAST”, the first high-performance linear algebra-based graph framework on NVIDIA GPUs that is open-source. The results show that on a single GPU, GraphBLAST has on average at least an order of magnitude speedup over previous GraphBLAS implementations SuiteSparse and GBTL, comparable performance to the fastest GPU hardwired primitives and shared-memory graph frameworks Ligra and Gunrock, and better performance than any other GPU graph framework, while offering a simpler and more concise programming model.},
journal = {ACM Trans. Math. Softw.},
month = {feb},
articleno = {1},
numpages = {51},
keywords = {Graph algorithm, matrix multiply, GPU, sparse linear algebra}
}

@article{10.1145/3322125,
author = {Davis, Timothy A.},
title = {Algorithm 1000: SuiteSparse:GraphBLAS: Graph Algorithms in the Language of Sparse Linear Algebra},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0098-3500},
url = {https://doi.org/10.1145/3322125},
doi = {10.1145/3322125},
journal = {ACM Trans. Math. Softw.},
month = dec,
articleno = {44},
numpages = {25},
keywords = {sparse matrices, GraphBLAS, Graph algorithms}
}

@inproceedings{kenter2019invited,
  title={Invited Tutorial: OpenCL design flows for Intel and Xilinx FPGAs: Using common design patterns and dealing with vendor-specific differences},
  author={Kenter, Tobias},
  booktitle={FSP Workshop 2019; Sixth International Workshop on FPGAs for Software Programmers},
  pages={1--8},
  year={2019},
  organization={VDE}
}

@INPROCEEDINGS{6567546,
  author={K. {Shagrithaya} and K. {Kepa} and P. {Athanas}},
  booktitle={2013 IEEE 24th International Conference on Application-Specific Systems, Architectures and Processors}, 
  title={Enabling development of OpenCL applications on FPGA platforms}, 
  year={2013},
  volume={},
  number={},
  pages={26--30},
  doi={10.1109/ASAP.2013.6567546}}

@inproceedings{10.5555/3433701.3433800,
author = {Selvitopi, Oguz and Ekanayake, Saliya and Guidi, Giulia and Pavlopoulos, Georgios A. and Azad, Ariful and Bulu\c{c}, Ayd\i{}n},
title = {Distributed Many-to-Many Protein Sequence Alignment Using Sparse Matrices},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {Identifying similar protein sequences is a core step in many computational biology pipelines such as detection of homologous protein sequences, generation of similarity protein graphs for downstream analysis, functional annotation, and gene location. Performance and scalability of protein similarity search have proven to be a bottleneck in many bioinformatics pipelines due to increase in cheap and abundant sequencing data. This work presents a new distributed-memory software PASTIS. PASTIS relies on sparse matrix computations for efficient identification of possibly similar proteins. We use distributed sparse matrices for scalability and show that the sparse matrix infrastructure is a great fit for protein similarity search when coupled with a fully-distributed dictionary of sequences that allow remote sequence requests to be fulfilled. Our algorithm incorporates the unique bias in amino acid sequence substitution in search without altering basic sparse matrix model, and in turn, achieves ideal scaling up to millions of protein sequences.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {75},
numpages = {14},
location = {Atlanta, Georgia},
series = {SC '20}
}

@INPROCEEDINGS{8091098,
  author={J. {Kepner} and M. {Kumar} and J. {Moreira} and P. {Pattnaik} and M. {Serrano} and H. {Tufo}},
  booktitle={2017 IEEE High Performance Extreme Computing Conference (HPEC)}, 
  title={Enabling massive deep neural networks with the GraphBLAS}, 
  year={2017},
  volume={},
  number={},
  pages={1--10},
  doi={10.1109/HPEC.2017.8091098}}


@article{10.1145/2544174.2500595,
author = {McDonell, Trevor L. and Chakravarty, Manuel M.T. and Keller, Gabriele and Lippmeier, Ben},
title = {Optimising Purely Functional GPU Programs},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/2544174.2500595},
doi = {10.1145/2544174.2500595},
abstract = {Purely functional, embedded array programs are a good match for SIMD hardware, such as GPUs. However, the naive compilation of such programs quickly leads to both code explosion and an excessive use of intermediate data structures. The resulting slow-down is not acceptable on target hardware that is usually chosen to achieve high performance.In this paper, we discuss two optimisation techniques, sharing recovery and array fusion, that tackle code explosion and eliminate superfluous intermediate structures. Both techniques are well known from other contexts, but they present unique challenges for an embedded language compiled for execution on a GPU. We present novel methods for implementing sharing recovery and array fusion, and demonstrate their effectiveness on a set of benchmarks.},
journal = {SIGPLAN Not.},
month = sep,
pages = {49--60},
numpages = {12},
keywords = {data parallelism, arrays, gpgpu, array fusion, embedded language, dynamic compilation, sharing recovery, haskell}
}

@article{10.1145/3276489,
author = {Lei\ss{}a, Roland and Boesche, Klaas and Hack, Sebastian and P\'{e}rard-Gayot, Ars\`{e}ne and Membarth, Richard and Slusallek, Philipp and M\"{u}ller, Andr\'{e} and Schmidt, Bertil},
title = {AnyDSL: A Partial Evaluation Framework for Programming High-Performance Libraries},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {OOPSLA},
url = {https://doi.org/10.1145/3276489},
doi = {10.1145/3276489},
abstract = {This paper advocates programming high-performance code using partial evaluation. We present a clean-slate programming system with a simple, annotation-based, online partial evaluator that operates on a CPS-style intermediate representation. Our system exposes code generation for accelerators (vectorization/parallelization for CPUs and GPUs) via compiler-known higher-order functions that can be subjected to partial evaluation. This way, generic implementations can be instantiated with target-specific code at compile time. In our experimental evaluation we present three extensive case studies from image processing, ray tracing, and genome sequence alignment. We demonstrate that using partial evaluation, we obtain high-performance implementations for CPUs and GPUs from one language and one code base in a generic way. The performance of our codes is mostly within 10%, often closer to the performance of multi man-year, industry-grade, manually-optimized expert codes that are considered to be among the top contenders in their fields.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {119},
numpages = {30},
keywords = {GPU computing, library design, high-performance, parallelization, partial evaluation, vectorization}
}

@article{10.1177/1094342011403516,
author = {Bulu\c{c}, Ayd\i{}n and Gilbert, John R},
title = {The Combinatorial BLAS: Design, Implementation, and Applications},
year = {2011},
issue_date = {November  2011},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {25},
number = {4},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342011403516},
doi = {10.1177/1094342011403516},
abstract = {This paper presents a scalable high-performance software library to be used for graph analysis and data mining. Large combinatorial graphs appear in many applications of high-performance computing, including computational biology, informatics, analytics, web search, dynamical systems, and sparse matrix methods. Graph computations are difficult to parallelize using traditional approaches due to their irregular nature and low operational intensity. Many graph computations, however, contain sufficient coarse-grained parallelism for thousands of processors, which can be uncovered by using the right primitives. We describe the parallel Combinatorial BLAS, which consists of a small but powerful set of linear algebra primitives specifically targeting graph and data mining applications. We provide an extensible library interface and some guiding principles for future development. The library is evaluated using two important graph algorithms, in terms of both performance and ease-of-use. The scalability and raw performance of the example applications, using the Combinatorial BLAS, are unprecedented on distributed memory clusters.},
journal = {Int. J. High Perform. Comput. Appl.},
month = nov,
pages = {496--509},
numpages = {14},
keywords = {sparse matrices, combinatorial BLAS, Betweenness centrality, combinatorial scientific computing, parallel graph library, Markov clustering, graph analysis, software framework, mathematical software}
}

@inproceedings{10.1145/3332466.3374507,
author = {Tyurin, Aleksey and Berezun, Daniil and Grigorev, Semyon},
title = {Optimizing GPU Programs by Partial Evaluation},
year = {2020},
isbn = {9781450368186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332466.3374507},
doi = {10.1145/3332466.3374507},
abstract = {While GPU utilization allows one to speed up computations to the orders of magnitude, memory management remains the bottleneck making it often a challenge to achieve the desired performance. Hence, different memory optimizations are leveraged to make memory being used more effectively. We propose an approach automating memory management utilizing partial evaluation, a program transformation technique that enables data accesses to be pre-computed, optimized, and embedded into the code, saving memory transactions. An empirical evaluation of our approach shows that the transformed program could be up to 8 times as efficient as the original one in the case of CUDA C na\"{\i}ve string pattern matching algorithm implementation.},
booktitle = {Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {431--432},
numpages = {2},
keywords = {GPU, CUDA, partial evaluation},
location = {San Diego, California},
series = {PPoPP '20}
}

@INPROCEEDINGS{7965105,
  author={Horn, William and Tanase, Gabriel and Yu, Hao and Pattnaik, Pratap},
  booktitle={2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
  title={A Linear Algebra-Based Programming Interface for Graph Computations in Scala and Spark}, 
  year={2017},
  volume={},
  number={},
  pages={653-659},
  doi={10.1109/IPDPSW.2017.142}}
