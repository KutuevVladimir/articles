@misc{yang2019graphblast,
    title={GraphBLAST: A High-Performance Linear Algebra-based Graph Framework on the GPU},
    author={Carl Yang and Aydin Buluc and John D. Owens},
    year={2019},
    eprint={1908.01407},
    archivePrefix={arXiv},
    primaryClass={cs.DC}
}

@article{Coimbra2021,
  doi = {10.1186/s40537-021-00443-9},
  url = {https://doi.org/10.1186/s40537-021-00443-9},
  year = {2021},
  month = apr,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {8},
  number = {1},
  author = {Miguel E. Coimbra and Alexandre P. Francisco and Lu{\'{\i}}s Veiga},
  title = {An analysis of the graph processing landscape},
  journal = {Journal of Big Data}
}

@INPROCEEDINGS{7761646,  author={Kepner, Jeremy and Aaltonen, Peter and Bader, David and Buluç, Aydin and Franchetti, Franz and Gilbert, John and Hutchison, Dylan and Kumar, Manoj and Lumsdaine, Andrew and Meyerhenke, Henning and McMillan, Scott and Yang, Carl and Owens, John D. and Zalewski, Marcin and Mattson, Timothy and Moreira, Jose},  booktitle={2016 IEEE High Performance Extreme Computing Conference (HPEC)},   title={Mathematical foundations of the GraphBLAS},   year={2016},  volume={},  number={},  pages={1-9},  doi={10.1109/HPEC.2016.7761646}}

% Boost compute library
@inproceedings{10.1145/2909437.2909454:boost:compute,
    author = {Szuppe, Jakub},
    title = {Boost.Compute: A Parallel Computing Library for C++ Based on OpenCL},
    year = {2016},
    isbn = {9781450343381},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2909437.2909454},
    doi = {10.1145/2909437.2909454},
    abstract = {Boost.Compute is a powerful C++ header-only template library for parallel computing based on OpenCL. It has a layered architecture and acts both as a thin C++ wrapper over the OpenCL API and as a feature-rich interface to high-level constructs that resemble the functionality of the STL and is just as easy to use.Through its template-based design, Boost.Compute seeks to further provide flexible OpenCL support in C++ projects via meta-programming. Meta-programming is not available in standard C-based OpenCL, and OpenCL C++ kernel language is currently only provisional. SYCL is intended as a prospective C++ abstraction layer over OpenCL, but it is still under development and requires a special compiler. Bolt and Thrust are C++-based parallel-computing libraries, but they are tied to specific hardware vendors. Boost.Compute on the other hand is a vendor-neutral solution for GPUs and multi-core CPUs that is available now, and it works with any standard C++ compiler and on all OpenCL platforms.Boost.Compute has been accepted for integration with the official Boost C++ libraries. With this step, and considering the large number of Boost users, usage of Boost.Compute and visibility of OpenCL among C++ developers is likely to increase. This technical presentation is therefore intended as a comprehensive overview of Boost.Compute for current and prospective users of the library and covers the library's overall architecture, its low-level and high-level functionality and advanced topics such as custom functions, closures and lambda expressions. The presentation also describes how a custom template-based OpenCL library can be designed on top of Boost.Compute. Examples are included throughout the presentation to aid in a better understanding. Among others, I will demonstrate how advanced features of the library can lead to a simple and efficient C++-only solution for BLAS calculations. The architectural presentation of the library will be followed by a presentation of current performance results of the library and a comparison with competing solutions. I will conclude the presentation with insights that I gained during Google Summer of Code '15 and my overall experience in contributing to Boost.Compute, which I hope to be of interest to the wider developer community.},
    booktitle = {Proceedings of the 4th International Workshop on OpenCL},
    articleno = {15},
    numpages = {39},
    keywords = {Parallel computing, Boost Compute, Boost C++ Libraries, C++, OpenCL},
    location = {Vienna, Austria},
    series = {IWOCL '16}
}

% Fast SpVSpM
@INPROCEEDINGS{7284398:spvspm,
  author={Yang, Carl and Wang, Yangzihao and Owens, John D.},
  booktitle={2015 IEEE International Parallel and Distributed Processing Symposium Workshop}, 
  title={Fast Sparse Matrix and Sparse Vector Multiplication Algorithm on the GPU}, 
  year={2015},
  volume={},
  number={},
  pages={841-847},
  doi={10.1109/IPDPSW.2015.77}}

% Push-pull efficient
@misc{https://doi.org/10.48550/arxiv.1804.03327:pushpull,
  doi = {10.48550/ARXIV.1804.03327},
  url = {https://arxiv.org/abs/1804.03327},
  author = {Yang, Carl and Buluc, Aydin and Owens, John D.},
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Implementing Push-Pull Efficiently in GraphBLAS},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


% GPU Merge path
@inproceedings{inproceedings:gpu_merge_path,
  author = {Green, Oded and McColl, Robert and Bader, David A.},
  title = {GPU Merge Path: A GPU Merging Algorithm},
  year = {2012},
  isbn = {9781450313162},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2304576.2304621},
  doi = {10.1145/2304576.2304621},
  abstract = {Graphics Processing Units (GPUs) have become ideal candidates for the development of fine-grain parallel algorithms as the number of processing elements per GPU increases. In addition to the increase in cores per system, new memory hierarchies and increased bandwidth have been developed that allow for significant performance improvement when computation is performed using certain types of memory access patterns.Merging two sorted arrays is a useful primitive and is a basic building block for numerous applications such as joining database queries, merging adjacency lists in graphs, and set intersection. An efficient parallel merging algorithm partitions the sorted input arrays into sets of non-overlapping sub-arrays that can be independently merged on multiple cores. For optimal performance, the partitioning should be done in parallel and should divide the input arrays such that each core receives an equal size of data to merge.In this paper, we present an algorithm that partitions the workload equally amongst the GPU Streaming Multi-processors (SM). Following this, we show how each SM performs a parallel merge and how to divide the work so that all the GPU's Streaming Processors (SP) are utilized. All stages in this algorithm are parallel. The new algorithm demonstrates good utilization of the GPU memory hierarchy. This approach demonstrates an average of 20X and 50X speedup over a sequential merge on the x86 platform for integer and floating point, respectively. Our implementation is 10X faster than the fast parallel merge supplied in the CUDA Thrust library.},
  booktitle = {Proceedings of the 26th ACM International Conference on Supercomputing},
  pages = {331–340},
  numpages = {10},
  keywords = {graphics processors, measurement of multiple-processor systems, parallel algorithms, parallel systems},
  location = {San Servolo Island, Venice, Italy},
  series = {ICS '12}
}

@article{10.1145/3322125,
author = {Davis, Timothy A.},
title = {Algorithm 1000: SuiteSparse:GraphBLAS: Graph Algorithms in the Language of Sparse Linear Algebra},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {4},
issn = {0098-3500},
url = {https://doi.org/10.1145/3322125},
doi = {10.1145/3322125},
journal = {ACM Trans. Math. Softw.},
month = dec,
articleno = {44},
numpages = {25},
keywords = {sparse matrices, GraphBLAS, Graph algorithms}
}

@INPROCEEDINGS{7529957,
  author={Zhang, Peter and Zalewski, Marcin and Lumsdaine, Andrew and Misurda, Samantha and McMillan, Scott},
  booktitle={2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 
  title={GBTL-CUDA: Graph Algorithms and Primitives for GPUs}, 
  year={2016},
  volume={},
  number={},
  pages={912-920},
  doi={10.1109/IPDPSW.2016.185}}

@inproceedings{10.1145/2600212.2600227,
author = {Khorasani, Farzad and Vora, Keval and Gupta, Rajiv and Bhuyan, Laxmi N.},
title = {CuSha: Vertex-Centric Graph Processing on GPUs},
year = {2014},
isbn = {9781450327497},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600212.2600227},
doi = {10.1145/2600212.2600227},
abstract = {Vertex-centric graph processing is employed by many popular algorithms (e.g., PageRank) due to its simplicity and efficient use of asynchronous parallelism. The high compute power provided by SIMT architecture presents an opportunity for accelerating these algorithms using GPUs. Prior works of graph processing on a GPU employ Compressed Sparse Row (CSR) form for its space-efficiency; however, CSR suffers from irregular memory accesses and GPU underutilization that limit its performance. In this paper, we present CuSha, a CUDA-based graph processing framework that overcomes the above obstacle via use of two novel graph representations: G-Shards and Concatenated Windows (CW). G-Shards uses a concept recently introduced for non-GPU systems that organizes a graph into autonomous sets of ordered edges called shards. CuSha's mapping of GPU hardware resources on to shards allows fully coalesced memory accesses. CW is a novel representation that enhances the use of shards to achieve higher GPU utilization for processing sparse graphs. Finally, CuSha fully utilizes the GPU power by processing multiple shards in parallel on GPU's streaming multiprocessors. For ease of programming, CuSha allows the user to define the vertex-centric computation and plug it into its framework for parallel processing of large graphs. Our experiments show that CuSha provides significant speedups over the state-of-the-art CSR-based virtual warp-centric method for processing graphs on GPUs.},
booktitle = {Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing},
pages = {239–252},
numpages = {14},
keywords = {graph representation, coalesced memory accesses, concatenated windows, g-shards, gpu},
location = {Vancouver, BC, Canada},
series = {HPDC '14}
}

@INPROCEEDINGS{7967137,  author={Pan, Yuechao and Wang, Yangzihao and Wu, Yuduo and Yang, Carl and Owens, John D.},  booktitle={2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},   title={Multi-GPU Graph Analytics},   year={2017},  volume={},  number={},  pages={479-490},  doi={10.1109/IPDPS.2017.117}}

% SuiteSparse Sparse Matrix Collection
@article{dataset:10.1145/2049662.2049663,
  author = {Davis, Timothy A. and Hu, Yifan},
  title = {The University of Florida Sparse Matrix Collection},
  year = {2011},
  issue_date = {November 2011},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {38},
  number = {1},
  issn = {0098-3500},
  url = {https://doi.org/10.1145/2049662.2049663},
  doi = {10.1145/2049662.2049663},
  abstract = {We describe the University of Florida Sparse Matrix Collection, a large and actively growing set of sparse matrices that arise in real applications. The Collection is widely used by the numerical linear algebra community for the development and performance evaluation of sparse matrix algorithms. It allows for robust and repeatable experiments: robust because performance results with artificially generated matrices can be misleading, and repeatable because matrices are curated and made publicly available in many formats. Its matrices cover a wide spectrum of domains, include those arising from problems with underlying 2D or 3D geometry (as structural engineering, computational fluid dynamics, model reduction, electromagnetics, semiconductor devices, thermodynamics, materials, acoustics, computer graphics/vision, robotics/kinematics, and other discretizations) and those that typically do not have such geometry (optimization, circuit simulation, economic and financial modeling, theoretical and quantum chemistry, chemical process simulation, mathematics and statistics, power networks, and other networks and graphs). We provide software for accessing and managing the Collection, from MATLAB™, Mathematica™, Fortran, and C, as well as an online search capability. Graph visualization of the matrices is provided, and a new multilevel coarsening scheme is proposed to facilitate this task.},
  journal = {ACM Trans. Math. Softw.},
  month = {dec},
  articleno = {1},
  numpages = {25},
  keywords = {performance evaluation, Graph drawing, sparse matrices, multilevel algorithms}
}

% La-Graph
@misc{szarnyas2021lagraph,
  title={LAGraph: Linear Algebra, Network Analysis Libraries, and the Study of Graph Algorithms}, 
  author={Gábor Szárnyas and David A. Bader and Timothy A. Davis and James Kitchen and Timothy G. Mattson and Scott McMillan and Erik Welch},
  year={2021},
  eprint={2104.01661},
  archivePrefix={arXiv},
  primaryClass={cs.MS}
}

% Graphalytics benchmarks
@misc{Graphalytics:iosup2021ldbc,
  title={The LDBC Graphalytics Benchmark}, 
  author={Alexandru Iosup and Ahmed Musaafir and Alexandru Uta and Arnau Prat Pérez and Gábor Szárnyas and Hassan Chafi and Ilie Gabriel Tănase and Lifeng Nai and Michael Anderson and Mihai Capotă and Narayanan Sundaram and Peter Boncz and Siegfried Depner and Stijn Heldens and Thomas Manhardt and Tim Hegeman and Wing Lung Ngai and Yinglong Xia},
  year={2021},
  eprint={2011.15028},
  archivePrefix={arXiv},
  primaryClass={cs.DC}
}