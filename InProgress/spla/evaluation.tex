\section{Evaluation}

For performance analysis of the proposed solution, we evaluated a few most common graph algorithms using real-world sparse matrix data. 
As a baseline for comparison we chose LAGraph~\cite{szarnyas2021lagraph} in connection with SuiteSparse~\cite{10.1145/3322125} as a CPU tool, Gunrock~\cite{7967137} and GraphBLAST~\cite{yang2019graphblast} as a Nvidia GPU tools. 
Also, we tested algorithms on several devices with distinct OpenCL vendors in order to validate the portability of the proposed solution. 
In general, these evaluation intentions are summarized in the following research questions. 

\vspace{0.2cm}
\begin{itemize}
    \item[\textbf{RQ1}] What is the performance of the proposed solution relative to existing tools for GPU analysis?

    \item[\textbf{RQ2}] What is the performance of the proposed solution on various devices vendors and OpenCL runtimes?

    \item[\textbf{RQ3}] What is the performance of the proposed solution on integrated GPU compared to existing CPU tool for analysis?
\end{itemize}

\subsection{Evaluation Setup}

\textbf{For evaluation of RQ1}, we use a PC with Ubuntu 20.04 installed, which has 3.40Hz Intel Core i7-6700 4-core CPU, DDR4 64Gb RAM, Intel HD Graphics 530 integrated GPU, and Nvidia GeForce GTX 1070 dedicated GPU with 8Gb on-board VRAM. 
\textbf{For evaluation of RQ2}, we use a PC with Ubuntu 22.04 installed, which has 4.70Hz AMD Ryzen 9 7900x 12-core CPU, DDR4 128 GB RAM, AMD GFX1036 integrated GPU, and either Intel Arc A770 flux dedicated GPU with 8GB on-board VRAM or AMD Radeon Vega Frontier Edition dedicated GPU with 16GB on-board VRAM.
\textbf{For evaluation of RQ3}, the first PC with Intel CPU and integrated GPU and the second PC with AMD CPU and integrated GPU are used.

Programs using CUDA were compiled with GCC v8.4.0 and Nvidia NVCC v10.1.
Release mode and maximum optimization level were enabled for all tested programs. 
Data loading time, preparation, format transformations, and host-device initial communications are excluded from time measurements. 
All tests are averaged across 10 runs. The deviation of measurements does not exceed the threshold of 10 percent. Additional warm-up run for each test execution is excluded from measurements.

\subsection{Graph Algorithms}

For preliminary study \textit{breadth-first search} (BFS), \textit{single-source shortest paths} (SSSP), \textit{page rank} (PR) and \textit{triangles counting} (TC) algorithms were chosen.
Implementation of those algorithms is used from official examples packages of tested libraries with default parameters. Compared tools are allowed to make any optimizations as long as the result remains correct.
The graph vertex with index 1 is set as the initial traversal vertex in the algorithms BFS and SSSP for all tested instruments and all tested devices.

\subsection{Dataset}

Thirteen matrices with graph data were selected from the Sparse Matrix Collection at University of Florida~\cite{dataset:10.1145/2049662.2049663}. 
Information about graphs is summarized in Table~\ref{dataset:info}. 
The dataset is converted to undirected graphs. 
Self-loops and duplicated edges are removed. 
Average, sd and max metrics relate to out degree property of the vertices.
For SSSP weights are initialized using pseudo-random generator with uniform $[0, 1]$ distribution of floating-point values.

Graphs are roughly divided into two groups. 
The first group represents relatively dense graphs, where the number of edges per node is sufficient on average to effectively load the GPU with useful work. 
The second group represents relatively sparse graphs, where the average vertex degree is below the typical GPU vector register size, and the search depth reaches hundreds of hoops. 
Graphs are sorted in ascending order by the number of vertices within each group.

\begin{table}[tbp]
\caption{Dataset description.} 
\begin{center}
    \rowcolors{2}{black!2}{black!10}
    \begin{tabular}{|l|r|r|r|r|r|}
    \hline
    Graph&Vertices&Edges&Avg&Sd&Max\\
    \hline
    \hline
    coAuthorsCit&227.3K&1.6M&7.2&10.6&1.4K\\
    coPapersDBLP&540.5K&30.5M&56.4&66.2&3.3K\\
    amazon2008&735.3K&7.0M&9.6&7.6&1.1K\\
    hollywood2009&1.1M&112.8M&98.9&271.9&11.5K\\
    comOrkut&3.1M&234.4M&76.3&154.8&33.3K\\
    citPatents&3.8M&33.0M&8.8&10.5&793.0\\
    socLiveJournal&4.8M&85.7M&17.7&52.0&20.3K\\
    indochina2004&7.4M&302.0M&40.7&329.6&256.4K\\
    \hline
    belgiumosm&1.4M&3.1M&2.2&0.5&10.0\\
    roadNetCA&2.0M&5.5M&2.8&1.0&12.0\\
    rggn222s0&4.2M&60.7M&14.5&3.8&36.0\\
    rggn223s0&8.4M&127.0M&15.1&3.9&40.0\\
    roadcentral&14.1M&33.9M&2.4&0.9&8.0\\
    \hline
    \end{tabular}
    \label{dataset:info}
\end{center}
\end{table}

\subsection{Results Summary}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.9\linewidth]{plots/rq1_rel_bfs.pdf}
\caption{Performance of GPU tools in BFS algorithm compared to LaGraph. Logarithmic scale is used.}
\label{fig:rq1_bfs}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.9\linewidth]{plots/rq1_rel_sssp.pdf}
\caption{Performance of GPU tools in SSSP algorithm compared to LaGraph. Logarithmic scale is used.}
\label{fig:rq1_sssp}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.9\linewidth]{plots/rq1_rel_pr.pdf}
\caption{Performance of GPU tools in PR algorithm compared to LaGraph. Logarithmic scale is used.}
\label{fig:rq1_pr}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.9\linewidth]{plots/rq1_rel_tc.pdf}
\caption{Performance of GPU tools in TC algorithm compared to LaGraph. Logarithmic scale is used.}
\label{fig:rq1_tc}
\end{figure}


\begin{figure}[tbp]
\centering
\includegraphics[width=0.9\linewidth]{plots/rq2_cores_bfs.pdf}
\caption{Performance of Spla library in BFS algorithm on different devices relative to number of compute cores. Logarithmic scale is used.}
\label{fig:rq2_bfs}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.9\linewidth]{plots/rq2_cores_sssp.pdf}
\caption{Performance of Spla library in SSSP algorithm on different devices relative to number of compute cores. Logarithmic scale is used.}
\label{fig:rq2_sssp}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.9\linewidth]{plots/rq2_cores_pr.pdf}
\caption{Performance of Spla library in PR algorithm on different devices relative to number of compute cores. Logarithmic scale is used.}
\label{fig:rq2_pr}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.9\linewidth]{plots/rq2_cores_tc.pdf}
\caption{Performance of Spla library in TC algorithm on different devices relative to number of compute cores. Logarithmic scale is used.}
\label{fig:rq2_tc}
\end{figure}


\begin{figure}[tbp]
\centering
\includegraphics[width=0.9\linewidth]{plots/rq3_int_bfs.pdf}
\caption{Performance of Spla library in BFS on integrated GPU compared to LaGraph on the same chip. Logarithmic scale is used.}
\label{fig:rq3_bfs}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.9\linewidth]{plots/rq3_int_sssp.pdf}
\caption{Performance of Spla library in SSSP on integrated GPU compared to LaGraph on the same chip. Logarithmic scale is used.}
\label{fig:rq3_sssp}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.9\linewidth]{plots/rq3_int_pr.pdf}
\caption{Performance of Spla library in PR on integrated GPU compared to LaGraph on the same chip. Logarithmic scale is used.}
\label{fig:rq3_pr}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=0.9\linewidth]{plots/rq3_int_tc.pdf}
\caption{Performance of Spla library in TC on integrated GPU compared to LaGraph on the same chip. Logarithmic scale is used.}
\label{fig:rq3_tc}
\end{figure}

Table~\ref{rq1_table} presents results of the evaluation and compares the performance of Spla against other Nvidia GPU tools and uses as a baseline LaGraph CPU tool. 
Table~\ref{rq2_table} presents result of the portability analysis of the proposed solution. It shows performance of the proposed solution on discrete GPUs of distinct vendors.
Table~\ref{rq3_table} present result of per-device comparison of Spla library running on integrated GPU and CPU LaGraph running on the same chip.

Cell left empty with \textit{none} if tested tool failed to analyze graph due to \textit{out of memory} exception.\\

\textit{RQ1. What is the performance of the proposed solution relative to existing tools for GPU analysis?}\\

In general, Spla shows very acceptable performance in all algorithms, running with comparable speed to its nearest competitor, GraphBLAST. Also proposed library does not not suffer from memory issues on some large graphs such as indochina or orkut. Spla is consistently several times faster than LaGraph, overcoming it up to $12\times$ in some cases. Gunrock, the fastest GPU framework for analysis, dominates the overall performance and only suffers in a PR algorithm.

Taking a closer look at Fig.~\ref{fig:rq1_bfs}, Spla BFS shows comparable to GraphBLAST performance in most runs. Spla has good speed at relatively dense graphs with high vertex degree and small depth of the search, what allows to saturate GPU with a work better. However, the performance degrades in network and road graphs with small front of the search and large diameter, what cause a lot of iterations. Thus, both Spla and GraphBLAST suffer from the overhead of kernel launches and relatively small amount of the work for a GPU. SSSP in Fig.~\ref{fig:rq1_sssp} shares with BFS the same picture in general. However, Spla behaves here slightly better than GraphBLAST, running up to $36\times$ faster at some extreme cases.

For a PR in Fig.~\ref{fig:rq1_pr}, Spla and GraphBLAST show the best performance, except cases with GraphBLAST memory issues. Both tools are faster than Gunrock in average reaching up to $20\times$ and more relative speedup. This performance can be motivated by the usage of mxv operation as core primitive, which is computationally intensive and has good work load balance. Spla suffers a bit in case of lower-degree graphs due to lack of more precise balance for small matrix rows.

Finally, detailed TC results are shown in Fig.~\ref{fig:rq1_tc}. Gunrock dominates performance as well, except two sparse road graphs where it has significant performance drop down. Spla and GraphBLAST have comparable results. However, GraphBLAST slightly faster nearly in all runs. Both tools use the same approach for mxmT implementation. However, Spla may encounter some OpenCL overhead or lack of precise performance tuning.\\

\textit{RQ2. What is the performance of the proposed solution on various devices vendors and OpenCL runtimes?}\\

Spla successfully launches and workes on the GPU of distinct vendors, including Intel, AMD and Nvidia. It shows promising performance and demonstrated scalability in relation to the number of computing cores. Fig.~\ref{fig:rq2_bfs} for BFS, Fig.~\ref{fig:rq2_sssp} for SSSP, Fig.~\ref{fig:rq2_pr} and Fig.~\ref{fig:rq2_tc} depict the edge per second throughput per a GPU core for all devices. This metric is quite predictable for the same graphs. This can be seen if one takes into account the overall shape of the figures for BFS, SSSP and PR as a whole.

In general, Spla on Nvidia shows better average performance, especially for sparser graphs with smaller degree per row. Nvidia OpenCL driver features faster memory allocations and has less overhead on a frequent kernel launches. Spla on Intel runtime lags a bit behind Nvidia, but performs better on some TC runs as shown in Fig.~\ref{fig:rq2_tc}. Spla performance on AMD is acceptable. However, better tuning and further polishing are required.\\

\textit{RQ3. What is the performance of the proposed solution on integrated GPU compared to existing CPU tool for analysis?}\\

Result of detailed comparison are shown in Fig.~\ref{fig:rq3_bfs} for BFS, Fig.~\ref{fig:rq3_sssp} for SSSP, Fig.~\ref{fig:rq3_pr} for PR and Fig.~\ref{fig:rq3_tc} for TC. These figures depict Spla relative to LaGraph speedup on the same chip, where Spla is running on integrated GPU part and LaGraph is running on multi-core CPU part. 

In general, LaGraph shows better performance for both CPUs, especially on a new powerful AMD Ryzen with 12 cores. The difference in a speed is extremely dramatic in BFS and SSSP algorithms. For a PR algorithm the picture is slightly better. Spla shows up to $10\times$ speedup. PR algorithm tends to be more computationally intensive, so difference to BFS and SSSP is reasonable. For TC Spla performs better only for Intel device, having in some cases conservative $2\times$ speedup. 



